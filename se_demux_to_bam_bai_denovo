#!/usr/bin/env nextflow
//=============================================================================
//                              se_demux_to_bam_bai_denovo
//                                  J. Harrison
//  A Nextflow workflow that takes single-end reads, filters and demultiplexes,
//  generates a de novo assembly, and aligns reads to the assembly. The output
//  of this workflow are bam and bai files for each sample.
//
//  Usage: nextflow se_demux_to_bam_bai_denovo --in --key --path_out --dbpath --mode --threshold
//  Example: nextflow gbs_nextflow --in data/sample1.fastq --key data/samplekey.csv
//   --path_out ./data/ --dbpath ./contaminants/ --mode denovo --threshold 4
//
//
//  "in" - is a path to a directory with unzipped single-end reads
//  from a RADseq experiment. Reads should be fastq formatted.
//  "key" -- The demultiplexing key, which should have three columns:
//  well, sequence, sample. See example data to view expected formatting.
//  "path_out" - The output path, which can be wherever you want the files output by
//  this workflow to be written.
//  "dbpath" -is the path to a directory with bowtie formatted contaminant
//  databases.
//  "mode" - is the assembly
//  "threshold" - is the number of times a sequence must be observed to be
//                included in the assembly.
//
// Example call: nextflow run se_demux_to_bam_bai_denovo --in ./data/sample1.fastq
// --key data/samplekey.csv --dbpath ./contaminants/ --path_out ./data/ --mode denovo --threshold 4
//
// Each process will be explained at the beginning of its invocation.
//
// To see where time is spent use the workflow report feature of nextflow, e.g.:
// nextflow run <pipeline name> -with-report [file name]
//
// To see what commands are run, which is wise to make sure this does what it
// should, use:
//                  nextflow log "name" -f script
// Where name is the name of the run that is auto-assigned by NF
// (i.e., adjective_famousLastname). You can see all runs by typing nextflow log
// this will let you figure out which run to look at in more depth.
//
// Developer notes:
// 1. future modifications to persist zipped files through the
// workflow would be helpful, currently zipped input fastqs will not work.
// 2. The 'build index' process will need modified if a reference option is added
// 3. Stdout is not written to a file, nor is there any arithmetic to sum results
// from split up files. E.g., summing the numbers of reads removed during filtering
// would be useful.


//=============================================================================
//                            Input and log.info
//=============================================================================


outputdir = file(params.path_out) //specify the output directory
dbdir = file(params.dbpath) //specify the reference database path
params.mode = 'denovo'
key = file(params.key) //specify that the demux key is a file
params.threshold = 4

log.info """\
         G B S - N F   P I P E L I N E
         ===================================
         Input fastq file: ${params.in}
         Demultiplexing key: $key
         Output directory: $outputdir
         Contaminant database path: $dbdir
         Mode (should be either denovo or reference): ${params.mode}
         Threshold (sequence depth to be included in assembly): ${params.threshold}
         """
         .stripIndent()

// Define the channel for fastqs that is used to handle multiple files in parallel
// IMPORTANT: using collect to make this a value channel, so it can be reused.

fastq_raw = Channel.fromPath(params.in)
                      .splitFastq( by: 1000, file: true)

fastq_wholeFile = Channel.fromPath(params.in).collect()
//fastq_raw = Channel.fromPath(params.in).collect()

//=============================================================================
//                            Begin processes
//=============================================================================

process extractMachineName {
  // Extract the name of the sequencing machine from the input files.
  // This name is used during demultiplexing
  input:
    path fastq_file from fastq_wholeFile

  output:
      stdout machine_name

  script:
      """
      head -n 1 $fastq_file | cut -d: -f1 | sed 's/@//'
      """
  }
//======================================================
process clean {
  // Remove common contaminants from fastq(s) using tapioca script
  // see: https://github.com/ncgr/tapioca

  input:
    path fastq_file from fastq_raw
    val x from machine_name

  output:
      path 'out.fastq' into clean_out
      stdout ch1                           //capture STDOUT

  // Note that Nextflow looks in the bin folder in the dir it is run from.

  // We run tapioca, which does a bowtie alignment to determine sequences that
  // match contaminants. We append (>>) the output of each query to a hits file
  // that will be used to scrub the input fastq.

  script:
  """
  echo "Number of reads in $fastq_file"
  grep -c "^@" $fastq_file

  tap_contam_analysis --db ${dbdir}/phix174 --pct 80 ${fastq_file} > hits.txt
  echo "PhiX filtering completed for ${fastq_file}"
  tap_contam_analysis --db ${dbdir}/illumina_oligos --pct 20 ${fastq_file} >> hits.txt
  echo "Illumina adapter filtering completed for ${fastq_file}"
  tap_contam_analysis --db ${dbdir}/ecoli-k-12 --pct 80 ${fastq_file} >> hits.txt
  echo "E. coli filtering completed for ${fastq_file}"
  # Originally, scrubbing done via fqu-cull. But that program requires
  # compilation, which has led to cryptic problems on my system due to the
  # declaration of the sleep command.
  # cat ${fastq_file} | fqu_cull -r hits.txt
  # New solution uses tools basic to Linux.
  # See http://thegenomefactory.blogspot.com/2012/05/cool-use-of-unix-paste-with-ngs.html
  # and https://github.com/lh3/seqtk/issues/62
  # Paste is dope
  # The v flag to grep inverts the search. The F flag makes the input lines
  # a literal query, thus avoiding the regex metacharacters that would be present
  # in a fastq file. The f flag means the input queries are in a file.
  cut -f 1 hits.txt > hitheader
  cat ${fastq_file} | paste - - - - | grep -v -F -f hitheader | tr "\t" "\n" > out.fastq
  """
}

//======================================================
process filter {
  // Remove reads that have more than one expected error
  input:
    path fastq_file from clean_out

  output:
      path '*' into filter_out
      stdout ch2                           //capture STDOUT

  script:
      """
      vsearch --fastq_filter ${fastq_file} --fastq_maxee 1 --fastqout ${fastq_file}cleaned

      echo "Number of reads after filtering in ${fastq_file}cleaned"
      grep -c "^@" ${fastq_file}cleaned
      """
  }
//======================================================
process demux {
  // Take fastq(s) as input and demultiplex them using a perl script written
  // by C. Alex Buerkle and Z. Gompert. This script can handle variable
  // length barcodes from 8 to 10 bases long and allows for one base mismatch
  // correction.
  // We publish the output of this process to our output directory.
  // This copies the output from the working directory that Nextflow builds
  // the default is to make a symbolic link. However, if one wants to delete
  // the working directory then copying these intermediate outputs is a
  // way to retain them.

   publishDir "$params.path_out", pattern: "parsereport*", mode: 'copy'

  input:
    path fastq_file from filter_out
    val machine_name from machine_name

  output:
    path 'parsed_out.fastqcleaned' into demux_out
    //Need to just output the demuxed fastqs here, not the report

  script:
  """
  parse_barcodes768.pl ${key} ${fastq_file} $machine_name
  """
}

demux_out
  .collectFile(name: file("${outputdir}demuxTest.txt"))
  .set {
     demux_cat
   }

//======================================================
process split_files {
  // Take demultiplexed fastq and split into files for each sample
  // I shifted to a Python script that I found online and modified because
  // Python is easier to read than Perl.

  // Copy the split files to the output directory. In the future this might
  // be better implemented as an option, as it will eat up disk and increase
  // runtime.

   publishDir "$params.path_out/split_fastqs/", pattern: "*", mode: 'copy'

  input:
    each fastq_file from demux_cat

  output:
    path '*fastq' into split_out

  script:
  """
  split_by_header_nofolder.py ${fastq_file}
  rm ${outputdir}demuxTest.txt
  """
}

// Duplicate this channel so we can use it multiple times.
split_out.into {split_out1; split_out2}

//======================================================
process dereplicate {
  // Find all unique sequences in each sample.

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    each fastq_file from split_out1.collect()

  output:
    path '*' into derep_out

  //Note that I use 'shell' instead of 'script' here so that it is a bit easier
  // to mix bash and nextflow variables. Note that the nextflow variables
  // are preceded by a ! and the code block must be inside three single quotes
  // instead of three double quotes.

  shell:
  '''
  outname=$(basename !{fastq_file})
  vsearch --fastx_uniques !{fastq_file} --sizein --sizeout --fastaout ${outname}derep
  '''
}

//======================================================
//Join all the outputs of derep into a new file and turn that into a channel.
// Currently this outputs to the directory from which Nextflow is run, which I delete.
// Other solutions for this map-reduce problem seem clunkier

derep_out
  .collectFile(name: file("${outputdir}derep.txt"))
  .set {
     join_channel
   }

//======================================================
process remove_uncommon {
  // Dereplicate the combined, previously dereplicated files and keep
  // those sequences that occur N or more times (user defined as --threshold).

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    file fastq_file from join_channel

  output:
    path 'for_assembly.fasta' into remove_uncommon_out

  script:
  """
  cat ${fastq_file} | vsearch --fastx_uniques - --sizein --sizeout --fastaout out.fa
  vsearch --fastx_filter out.fa --minsize ${params.threshold} --sizein --sizeout --fastaout for_assembly.fasta
  rm ${outputdir}derep.txt
  """
}
//======================================================
process assemble {
  // Use cd-hit to perform a de novo assembly. Functionality will be added
  // for reference-based assembly at a later time.
  // Work by LaCava et al. 2020 suggests that cd-hit is the best assembler
  // for denovo work when variants are present.

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    file fastq_file from remove_uncommon_out

  output:
    path 'out' into assemble_out

  shell:
    '''
    # M means memory limit, zero means unlimited
    # T means threads, 0 means all available cpus should be used
    # c means "sequence identity threshold, default 0.9
 	  #   this is the default cd-hit's "global sequence identity" calculated as:
 	  #   number of identical amino acids or bases in alignment
 	  #   divided by the full length of the shorter sequence
    # https://stackoverflow.com/questions/7815553/iterate-through-fasta-entries-and-rename-duplicates
    cd-hit-est -i !{fastq_file} -o denovoAssembly_0.9 -M 0 -T 0 -c 0.9
    awk '/^>/{gsub(/^>/,">Seq"i++"");}1' denovoAssembly_0.9  > out
    '''
  }

//======================================================
process build_index {
  // Use bwa to make an index out of our denovo genome.
  // For bwa help see: http://bio-bwa.sourceforge.net/bwa.shtml

  // TODO: once a reference option is added to this worklow then that reference
  // will need to be passed in here and turned into an index

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    file genome from assemble_out

  output:
    path "*.{amb,ann,bwt,pac,sa}" into build_index_out

  script:
  """
  # -p is the prefix for the index
  bwa index -p "index" $genome
  """
  }
//======================================================
  process align_samples_genome {
    // Align samples to the newly indexed genome
    // Note that the original script to implement this step was runbwa.pl
    // I use Nextflow's file handling here instead of the perl wrapper.
    // Also, I changed from bwa aln to bwa mem because the latter is known
    // to have a higher mapping rate.

    // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

    input:
      each fastq_file from split_out2.collect()
      path indexFiles from build_index_out

    output:
      path '*' into align_samples_out

    shell:
    '''
    # From manual: Align 70bp-1Mbp query sequences with the BWA-MEM algorithm.
    # Briefly, the algorithm works by seeding alignments with maximal exact
    # matches (MEMs) and then extending seeds with the
    # affine-gap Smith-Waterman algorithm (SW).
    outname=$(basename !{fastq_file})
    bwa mem index !{fastq_file} > ${outname}aligned.sam
    '''
 }
 //======================================================
   process sam2bam {
      // Convert Sequence Alignment and Map (SAM) files to BAM files, which are
      // binary and thus smaller and easier to work with quickly.
      // Originally we accomplished this with the sam2bam perl script.
      // If you need a refresher on SAM/BAM then see this page:
      // https://www.zymoresearch.com/blogs/blog/what-are-sam-and-bam-files

      // publishDir "$params.path_out", pattern: "*", mode: 'copy'

   input:
     path samfile from align_samples_out

   output:
     path '*sorted.bam' into sam2bam

     script:
     """
     # Use "view" to convert from sam to bam (-b) and output (-o)
     # Use sort to sort by leftmost alignment coordinates
     # Note that cat didn't seem to work here (either with shell or script invocation).
     # Might be worth revisiting.
     samtools view -b -o ${samfile}.bam ${samfile}
     samtools sort ${samfile}.bam -o ${samfile}sorted.bam
     """
}

//======================================================
  process makebai {
     // Make index files for BAM files
     // Use index to make an index for the BAM file. By convention, indices are
     // suffixed with bai. The index explains which read is where in the file.

     // mode: move is a terminating process
     // publishDir "$params.path_out", pattern: "*", mode: 'move'

  input:
    path bamfile from sam2bam

  output:
      path("*") into bai_bam

    script:
    """
    samtools index $bamfile
    """
}

// print STDOUT
// Note that this happens during runtime and is not written anywhere.
// In the future, I want to write stdout to a file and do some parsing of it.
ch1.view()
ch2.view()
