#!/usr/bin/env nextflow
//=============================================================================
//              gbs_nextflow - A tool to ease RADseq bioinformatics
//                                  J. Harrison
//  Usage: nextflow gbs_nextflow --in --key --path_out --dbpath --mode --threshold
//  Example: nextflow gbs_nextflow --in data/sample1.fastq --key data/samplekey.csv
//   --path_out ./data/ --dbpath ./contaminants/ --mode denovo --threshold 4
//
//
//  "in" - is a path to a directory with unzipped single-end reads
//  from a RADseq experiment. Reads should be fastq formatted.
//  "key" -- The demultiplexing key, which should have three columns:
//  well, sequence, sample. See example data to view expected formatting.
//  "path_out" - The output path, which can be wherever you want the files output by
//  this workflow to be written.
//  "dbpath" -is the path to a directory with bowtie formatted contaminant
//  databases.
//  "mode" - is the assembly
//  "threshold" - is the number of times a sequence must be observed to be
//                included in the assembly.
//
// Example call: nextflow run gbs_nextflow --in ./data/sample1.fastq
// --key data/samplekey.csv --dbpath ./contaminants/ --path_out ./data/ --mode denovo --threshold 4
//
// Each process will be explained at the beginning of its invocation.
//
// To see where time is spent use the workflow report feature of nextflow, e.g.:
// nextflow run <pipeline name> -with-report [file name]
//
// To see what commands are run, which is wise to make sure this does what it
// should, use:
//                  nextflow log "name" -f script
// Where name is the name of the run that is auto-assigned by NF
// (i.e., adjective_famousLastname). You can see all runs by typing nextflow log
// this will let you figure out which run to look at in more depth.
//
// Developer notes: future modifications to allow zipped files could be useful
// The demulitplexing process could split the input fastqs first for a big
// speed gain.
//
// IMMEDIATE NEEDS:
// ****Need to pull the machine name from the file
// ****Need to add additional filtering to remove bogus reads.
// ***Need to fix the absolute path to cdhit I used. Path issues led me to this,
// quick fix since we will be running this other places anyway.
// ***The 'build index' process will need modified if a reference option is added
//=============================================================================

//Load input
// (see above for usage)

outputdir = file(params.path_out) //specify the output directory
dbdir = file(params.dbpath) //specify the reference database path
params.mode = 'denovo'
key = file(params.key) //specify that the demux key is a file
params.threshold = 4

log.info """\
         G B S - N F   P I P E L I N E
         ===================================
         Input fastq file: ${params.in}
         Demultiplexing key: $key
         Output directory: $outputdir
         Contaminant database path: $dbdir
         Mode (should be either denovo or reference): ${params.mode}
         Threshold (sequence depth to be included in assembly): ${params.threshold}
         """
         .stripIndent()

//======================================================
// Channel for fastqs that is used to handle multiple files in parallel
fastq_raw = Channel.fromPath(params.in)

//======================================================
// Begin processes

process clean {
  // Remove common contaminants from fastq(s) using tapioca script
  // see: https://github.com/ncgr/tapioca

  input:
    path fastq_file from fastq_raw

  output:
      path 'out.fastq' into clean_out
      stdout ch                           //capture STDOUT

  // Note that Nextflow looks in the bin folder in the dir it is run from.

  // We run tapioca, which does a bowtie alignment to determine sequences that
  // match contaminants. We append (>>) the output of each query to a hits file
  // that will be used to scrub the input fastq.

  script:
  """
  tap_contam_analysis --db ${dbdir}/phix174 --pct 80 ${fastq_file} > hits.txt
  echo "PhiX filtering completed for ${fastq_file}"

  tap_contam_analysis --db ${dbdir}/illumina_oligos --pct 20 ${fastq_file} >> hits.txt
  echo "Illumina adapter filtering completed for ${fastq_file}"

  tap_contam_analysis --db ${dbdir}/ecoli-k-12 --pct 80 ${fastq_file} >> hits.txt
  echo "E. coli filtering completed for ${fastq_file}"

  # Originally, scrubbing done via fqu-cull. But that program requires
  # compilation, which has led to cryptic problems on my system due to the
  # declaration of the sleep command.
  # cat ${fastq_file} | fqu_cull -r hits.txt

  # New solution uses tools basic to Linux.
  # See http://thegenomefactory.blogspot.com/2012/05/cool-use-of-unix-paste-with-ngs.html
  # and https://github.com/lh3/seqtk/issues/62
  # Paste is dope
  # The v flag to grep inverts the search. The F flag makes the input lines
  # a literal query, thus avoiding the regex metacharacters that would be present
  # in a fastq file. The f flag means the input queries are in a file.

  cut -f 1 hits.txt > hitheader
  cat ${fastq_file} | paste - - - - | grep -v -F -f hitheader | tr "\t" "\n" > out.fastq
  """
}
ch.view { print "$it" } //print stdout

//======================================================
process demux {
  // Take fastq(s) as input and demultiplex them using a perl script written
  // by C. Alex Buerkle and Z. Gompert. This script can handle variable
  // length barcodes from 8 to 10 bases long and allows for one base mismatch
  // correction.

  //

  // We publish the output of this process to our output directory.
  // This copies the output from the working directory that Nextflow builds
  // the default is to make a symbolic link. However, if one wants to delete
  // the working directory then copying these intermediate outputs is a
  // way to retain them.

  publishDir "$params.path_out", pattern: "parsereport*", mode: 'copy'

  input:
    path fastq_file from clean_out

  output:
    path 'parsed_out.fastq' into demux_out

  script:
  """
  parse_barcodes768.pl ${key} ${fastq_file} K00188
  """
}

//======================================================
process split_files {
  // Take demultiplexed fastq and split into files for each sample
  // I shifted to a Python script that I found online and modified because
  // Python is easier to read than Perl.

  // Copy the split files to the output directory. In the future this might
  // be better implemented as an option, as it will eat up disk and increase
  // runtime substantially.

  publishDir "$params.path_out/split_fastqs/", pattern: "*", mode: 'copy'

  input:
    path fastq_file from demux_out

  output:
    path '*fastq' into split_out

  script:
  """

  split_by_header_nofolder.py ${fastq_file}
  """
}

//IMPORTANT. We convert the split-out channel into a value channel here.
// Value channels can be reused, but queue channels cannot.

split_outCollected =  split_out
       .first()

//======================================================
process dereplicate {
  // Find all unique sequences in each sample.

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    each fastq_file from split_outCollected

  output:
    path '*' into derep_out

  //Note that I use 'shell' instead of 'script' here so that it is a bit easier
  // to mix bash and nextflow variables. Note that the nextflow variables
  // are preceded by a ! and the code block must be inside three single quotes
  // instead of three double quotes.

  shell:
  '''
  outname=$(basename !{fastq_file})
  vsearch --fastx_uniques !{fastq_file} --sizeout --fastaout ${outname}derep
  '''
}
//======================================================
//Join all the outputs of derep into a new file and turn that into a channel.
// Currently this outputs to the directory from which Nextflow is run.
// There is probably a better solution than this.
derep_out
  .collectFile(name: file('derep.txt'))
  .set {
     join_channel
   }

//======================================================
process remove_uncommon {
  // Dereplicate the combined, previously dereplicated files and keep
  // those sequences that occur N or more times (user defined as --threshold).

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    file fastq_file from join_channel

  output:
    path 'for_assembly.fasta' into remove_uncommon_out

  script:
  """
  cat ${fastq_file} | vsearch --fastx_uniques - --sizein --sizeout --fastaout out.fa
  vsearch --fastx_filter out.fa --minsize ${params.threshold} --fastaout for_assembly.fasta
  """
}
//======================================================
process assemble {
  // Use cd-hit to perform a de novo assembly. Functionality will be added
  // for reference-based assembly at a later time.
  // Work by LaCava et al. 2020 suggests that cd-hit is the best assembler
  // for denovo work when variants are present.

  // For troubleshooting ease
   publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    file fastq_file from remove_uncommon_out

  output:
    path 'out' into assemble_out

  shell:
    '''
    # M means memory limit, zero means unlimited
    # T means threads, 0 means all available cpus should be used
    # c means "sequence identity threshold, default 0.9
 	  #   this is the default cd-hit's "global sequence identity" calculated as:
 	  #   number of identical amino acids or bases in alignment
 	  #   divided by the full length of the shorter sequence

    # https://stackoverflow.com/questions/7815553/iterate-through-fasta-entries-and-rename-duplicates
    ~/builds/cdhit/cd-hit-est -i !{fastq_file} -o denovoAssembly_0.9 -M 0 -T 0 -c 0.9
    awk '/^>/{gsub(/^>/,">Seq"i++"");}1' denovoAssembly_0.9  > out
    '''
  }

//======================================================
process build_index {
  // Use bwa to make an index out of our denovo genome.
  // For bwa help see: http://bio-bwa.sourceforge.net/bwa.shtml

  // TODO: once a reference option is added to this worklow then that reference
  // will need to be passed in here and turned into an index

  // For troubleshooting ease
  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    file genome from assemble_out

  output:
    path "*.{amb,ann,bwt,pac,sa}" into build_index_out

  script:
  """
  # -p is the prefix for the index

  bwa index -p "index" $genome
  """
  }
//======================================================
  process align_samples_genome {
    // Align samples to the newly indexed genome
    // Note that the original script to implement this step was runbwa.pl
    // I use Nextflow's file handling here instead of the perl wrapper.
    // Also, I changed from bwa aln to bwa mem because the latter is known
    // to have a higher mapping rate.

  //  publishDir "$params.path_out", pattern: "*", mode: 'rellink'

    input:
      each fastq_file from split_outCollected
      path indexFiles from build_index_out

    output:
      path '*' into align_samples_out

    shell:
    '''
    # From manual: Align 70bp-1Mbp query sequences with the BWA-MEM algorithm.
    # Briefly, the algorithm works by seeding alignments with maximal exact
    # matches (MEMs) and then extending seeds with the
    # affine-gap Smith-Waterman algorithm (SW).

    outname=$(basename !{fastq_file})
    bwa mem index !{fastq_file} > ${outname}aligned.sam
    '''
 }
 //======================================================
   process sam2bam {
      // Convert Sequence Alignment and Map (SAM) files to BAM files, which are
      // binary and thus smaller and easier to work with quickly.
      // Originally we accomplished this with the sam2bam perl script.
      // If you need a refresher on SAM/BAM then see this page:
      // https://www.zymoresearch.com/blogs/blog/what-are-sam-and-bam-files

  // publishDir "$params.path_out", pattern: "*", mode: 'rellink'

   input:
     path samfile from align_samples_out

   output:
     path '*sorted.bam' into sam2bam

     script:
     """
     # Use "view" to onvert from sam to bam (-b) and output (-o)
     # Use sort to sort by leftmost alignment coordinates

     # Note that cat didn't seem to work here (either with shell or script invocation).
     # Might be worth revisiting.
     samtools view -b -o ${samfile}.bam ${samfile}
     samtools sort ${samfile}.bam -o ${samfile}sorted.bam
     """
}
//======================================================
  process makebai {
     // Make index files for BAM files
     // Use index to make an index for the BAM file. By convention, indices are
     // suffixed with bai. The index explains which read is where in the file.

  publishDir "$params.path_out", pattern: "*", mode: 'rellink'

  input:
    path bamfile from sam2bam

  output:
    path '*' into makebai

    script:
    """
    samtools index $bamfile
    """
}
